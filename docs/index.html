<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>T-FunS3D: Task-Driven Hierarchical Open-Vocabulary 3D Functionality Segmentation via Vision-Language Models</title>
  <style>
    /* Minimal styling — inspired by simple academic project pages */
    body { font-family: sans-serif; line-height: 1.6; margin: 2em auto; max-width: 800px; padding: 0 1em; }
    h1 { font-size: 2em; margin-bottom: 0.1em; }
    h2 { margin-top: 1.5em; color: #333; }
    img { max-width: 100%; height: auto; margin: 1em 0; }
    .authors, .resources { font-size: 0.9em; color: #555; }
    .footer { margin-top: 2em; font-size: 0.8em; color: #777; border-top: 1px solid #eee; padding-top: 1em; }
    a { color: #0645ad; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>

  <h1>T-FunS3D: Task-Driven Hierarchical Open-Vocabulary 3D Functionality Segmentation via Vision-Language Models</h1>
  <p class="authors">
    Jingkun Feng<sup>1</sup>, Reza Sabzevari<sup>1</sup><br />
    <sup>1</sup>p4MARS Lab, Faculty of Aerospace Engineering, Delft University of Technology
  </p>

  <p class="links">
    <a href="#abstract">Abstract</a> •
    <a href="#method">Method</a> •
    <a href="#results">Results</a> •
    <!-- <a href="#resources">Resources</a> -->
  </p>

  <h2 id="abstract">Abstract</h2>
  <p>
    Open-vocabulary 3D functionality segmentation enables robots to localize functional object components in 3D scenes. It is a challenging task that requires spatial understanding and task interpretation. Current open-vocabulary 3D segmentation methods primarily focus on object-level recognition, while scene-wide part segmentation methods attempt to segment the entire scene exhaustively, making them highly resource-intensive. However, such significant computational and storage capacities are typically not accessible on the majority of mobile robots. To address this challenge, we introduce T-FunS3D, a task-driven hierarchical open-vocabulary 3D functionality segmentation method that provides actionable perception for robotic applications. Given a task description, T-FunS3D identifies the most relevant instances in an open-vocabulary scene graph and extracts their functional components. Experiments on SceneFun3D demonstrate that T-FunS3D outperforms baseline methods in open-vocabulary 3D functionality segmentation, while achieving faster runtime and reduced memory usage. 
  </p>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/t-funs3d_teaser.png" alt="T-FunS3D Teaser" />
        <p></p>
        <!-- <h2 class="subtitle has-text-centered" style="margin-top: 30px;">
          <b>Fun3DU</b> is the first method designed for <b>functionality understanding and segmentation</b> in 3D scene. 
          Given an <b>high-resolution 3D scene</b>, a set of RGBD views showing the scene and a description of an action to perform, <b>Fun3DU segments the functional object</b>(s) that can be <b>used to carry out the specific action</b>. 
          Fun3DU relies on pre-train vision and language models, is <b>training-free</b> and does not require task-specific annotations.
        </h2> -->
      </div>
    </div>
  </section>

  <h2 id="method">Method</h2>
  <p>
    <b>T-FunS3D</b> is a novel method for <b>efficient open-vocabulary functionality understanding and segmentation</b> in 3D scene using <b>scene graph and VLM</b>. 
    Given an <b>point cloud</b> of a 3D scene, a set of RGBD views showing the scene along with the associated camera poses and a description of an action to perform, <b>T-FunS3D segments the functional object</b>(s) that can be <b>used to carry out the specific action</b>. 
    T-FunS3D relies on pre-trained vision and language models, is <b>training-free</b> and does not require task-specific annotations.
  </p>
  <h3 id="method_stage1">Stage I: Open-Vocabulary Scene Graph Construction</h3>
  <img src="./static/images/t-funs3d_stage1.png" alt="T-FunS3D Stage I" />
  <p>
    The entire pipeline consists of four main modules (A-D), split into two stages, including an instance-level scene graph construction stage (I) and a task-driven functionality segmentation stage (II).
    At stage I, (A) performs open-vocabulary instance segmentation by associating FG-CLIP [2] visual embeddings to class-agnostic instance segmentation from Mask3D [3]. We construct a scene graph of the featurized instances (B). 
    Note that stage I is executed only once per scene, assuming no distinct spatial changes in the environment, and the resulting scene graph is reused for different tasks at stage II.
    In addition, the constructed scene graph is compact, as it only contains object instances in nodes and their relationships in edges that are stored as embedding feature vectors.
    Along with the embeddings, the top-k selected views of each instance are also stored to facilitate efficient 2D mask extraction at stage II.
    Therefore, T-FunS3D is efficient in terms of both runtime and memory usage.
  </p>

  <h3 id="method_stage2">Stage II: Task-Driven Functionality Segmentation</h3>
  <img src="./static/images/t-funs3d_stage2.png" alt="T-FunS3D Stage II" />
  <p>
    Stage II begins, once a task is assigned. The free-form task description is decompsed into ontologies using Qwen3 [4] (C). 
    Based on the extracted information, we identify the contextual object in the scene graph (D) by computing the text-visual embedding similarity for candidate nodes and their corresponding edges. 
    Lastly, (E) aggregates 2D masks extracted by combining Molmo [5] with SAM [6] to obtain 3D segmentation of functional parts.
  </p>

  <h2 id="results">Quantitative Results</h2>
  <p>
    Quantitative results of T-FunS3D on the validation split of SceneFun3D [1].
    We evaluated our method's ability to segment functional parts according to task descriptions on the validation split of the SceneFun3D [1] dataset.
    We compared T-FunS3D with four baselines: OpenMask3D [7], LERF [8], OpenIns3D [9], and Fun3DU [10]. Among them, the first three are SOTA open-vocabulary 3D instance segmentation methods, while Fun3DU is the first method dedicated to functionality segmentation in 3D scenes.
    As shown in the table, T-FunS3D outperforms all baselines by a clear margin in terms of mAP50, mAP25, and mIoU, demonstrating its effectiveness in open-vocabulary 3D functionality segmentation.
  </p>
    <img src="static/images/quantitative.png" alt="Quantitative results" />

  <h2 id="results">Referring grounding</h2>
  <p>
    We evaluated the referring grounding performance of T-FunS3D for selected tasks from the validation split of SceneFun3D [1].
    The figure below shows the distribution of the spatial relations in the task descriptions provided in the validation split.
    The following table compares T-FunS3D with Fun3DU [10] in segmenting functional parts based on tasks that contains spatial relations.
  </p>
  <img src="static/images/relation distribution.png" alt="Distribution of spatial relations" />
  <img src="static/images/referring_grounding.png" alt="Referring grounding results" />

  <h2 id="results">Qualitative Results / Examples</h2>
  <p>
    Qualitative results of T-FunS3D on SceneFun3D [1].
    Point clouds around the functional parts are highlighted for better visualization: red points indicate predictions, blue points denote ground truth, and green points represent overlaps.
  </p>
  <img src="static/images/res_viz.png" alt="Qualitative results" />
<!-- 
  <h2 id="resources">Resources</h2>
  <ul>
    <li><a href="https://arxiv.org/…">Paper (arXiv)</a></li>
    <li><a href="https://github.com/yourname/yourrepo">Code (GitHub)</a></li>
    <li><a href="https://your-project-website.com">Project Website</a></li>
  </ul> -->

    <!-- related works. -->
    <section class="section">
      <div class="container is-max-desktop">
            <h2 class="title is-4" id="references">Related work</h2>
            <div class="content has-text-justified">
              <p>
                [1] Delitzas, Alexandros, et al. "Scenefun3D: Fine-grained functionality and affordance understanding in 3D scenes." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
              </p>
              <p>
                [2] Xie, Chunyu, et al. "FG-CLIP: Fine-Grained Visual and Textual Alignment." arXiv preprint arXiv:2505.05071 (2025).
              </p>
              <p>
                [3] Schult, Jonas, et al. "Mask3d: Mask transformer for 3d semantic instance segmentation." arXiv preprint arXiv:2210.03105 (2022).
              </p>
              <p>
                [4] Yang, An, et al. "Qwen3 technical report." arXiv preprint arXiv:2505.09388 (2025).
              </p>
              <p>
                [5] Deitke, Matt, et al. "Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models." Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.
              </p>
              <p>
                [6] Kirillov, Alexander, et al. "Segment anything." Proceedings of the IEEE/CVF international conference on computer vision. 2023.
              </p>
              <p>
                [7] Takmaz, Ayca, et al. "OpenMask3D: Open-Vocabulary 3D Instance Segmentation." Advances in Neural Information Processing Systems. 2024.
              </p>
              <p>
                [8] Kerr, Justin, et al. "Lerf: Language embedded radiance fields." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
              </p>
              <p>
                [9] Huang, Zhening, et al. "Openins3D: Snap and lookup for 3d open-vocabulary instance segmentation." European Conference on Computer Vision. 2025.
              </p>
              <p>
                [10] Corsetti, Jaime, et al. "Functionality understanding and segmentation in 3D scenes." Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.
              </p>
          </div>
      </div>
    </section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

